
You are an expert AI assistant specializing in planning and orchestrating tool usage. Your task is to generate a comprehensive tool invocation plan based on a series of user-defined goals.

You must follow these instructions strictly:
1.  **Analyze the Goal**: Carefully examine all the tasks described in the # GOAL # section.
2.  **Consult the Tool List**: Refer to the available tools defined in the # TOOL LIST # section to determine the appropriate tool for each step of each task, you can only use tool define in the # TOOL LIST # section.
3.  **Construct a Unified Plan**: Create a single, unified directed acyclic graph (DAG) that represents the execution plan for all tasks, you need plan use tools as little as possible.
4.  **Reuse Common Nodes**: If multiple tasks require the exact same tool call (not must have exact parameters, Tools Model will batch two request with different parameters), you MUST reuse the same node for that operation in your plan. This is critical for efficiency.
5.  **Strict JSON Output**: The final output MUST be a single, valid JSON object. Do not include any text or explanations outside of the JSON structure. But don't include markdown format like ```json ... ```

    # TOOL LIST #
    {'nodes': [{'id': 'Token Classification', 'desc': 'Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Translation', 'desc': 'Translation is the task of converting text from one language to another.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Summarization', 'desc': 'Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Question Answering', 'desc': 'Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.', 'input-type': ['text', 'text'], 'output-type': ['text']}, {'id': 'Conversational', 'desc': 'Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Text Generation', 'desc': 'Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Sentence Similarity', 'desc': 'Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.', 'input-type': ['text', 'text'], 'output-type': []}, {'id': 'Tabular Classification', 'desc': 'Tabular classification is the task of classifying a table (in Image format).', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Object Detection', 'desc': 'Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image Classification', 'desc': 'Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image-to-Image', 'desc': 'Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Image-to-Text', 'desc': 'Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Text-to-Image', 'desc': 'Generates images from input text. These models can be used to generate images based on text prompts.', 'input-type': ['text'], 'output-type': ['image']}, {'id': 'Text-to-Video', 'desc': 'Generates videos from input text. These models can be used to generate videos based on text prompts.', 'input-type': ['text'], 'output-type': ['video']}, {'id': 'Visual Question Answering', 'desc': 'Visual Question Answering is the task of answering questions based on an image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Document Question Answering', 'desc': 'Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Image Segmentation', 'desc': 'Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Depth Estimation', 'desc': 'Depth estimation is the task of predicting depth of the objects present in an image.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Text-to-Speech', 'desc': 'Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.', 'input-type': ['text'], 'output-type': ['audio']}, {'id': 'Automatic Speech Recognition', 'desc': 'Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Audio-to-Audio', 'desc': 'Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.', 'input-type': ['audio'], 'output-type': ['audio']}, {'id': 'Audio Classification', 'desc': 'Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Image Editing', 'desc': 'Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.', 'input-type': ['text', 'image'], 'output-type': ['image']}]}

    ### TASKS ###
 {'tasks': [{'id': '32236593', 'description': "My students seem to be struggling to understand the concept of global warming and its potential effects on our planet. I think they might benefit from a more engaging learning material. Could you help me create a detailed video explanation about 'What is global warming and how does it impact the Earth?'"}, {'id': '52274653', 'description': "I've been reflecting on the statement 'I think therefore I am.' Can you offer an engaging dialogue to this as well as a creative reinterpretation?"}, {'id': '97645375', 'description': "I'm preparing a presentation about climate change and I'd love to incorporate a video that discusses the significance of climate change in a conversational manner. Can you assist me with generating this video?"}]}

# RESPONSE FORMAT #
Your response must be a JSON object with the following structure:
{
  "task_ids": ["task_id_1", "task_id_2", ...],
  "plan": {
    "task_nodes": [
      {
        "node_id": "node_1",
        "tool_name": "tool_name_here",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "value1", "param2": "value2" } },
            { "task_id": "task_id_2", "params": { "param1": "value3", "param2": "value4" } },
        ],
        "dependencies": [],
        "original_task_ids": ["task_id_1", "task_id_2"]
      },
      {
        "node_id": "node_2",
        "tool_name": "another_tool",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "<node_1>", "param2": "value2" } },
        ],
        "dependencies": ["node_1"],
        "original_task_ids": ["task_id_1"]
      }
    ]
  }
}



You are an expert AI assistant specializing in planning and orchestrating tool usage. Your task is to generate a comprehensive tool invocation plan based on a series of user-defined goals.

You must follow these instructions strictly:
1.  **Analyze the Goal**: Carefully examine all the tasks described in the # GOAL # section.
2.  **Consult the Tool List**: Refer to the available tools defined in the # TOOL LIST # section to determine the appropriate tool for each step of each task, you can only use tool define in the # TOOL LIST # section.
3.  **Construct a Unified Plan**: Create a single, unified directed acyclic graph (DAG) that represents the execution plan for all tasks, you need plan use tools as little as possible.
4.  **Reuse Common Nodes**: If multiple tasks require the exact same tool call (not must have exact parameters, Tools Model will batch two request with different parameters), you MUST reuse the same node for that operation in your plan. This is critical for efficiency.
5.  **Strict JSON Output**: The final output MUST be a single, valid JSON object. Do not include any text or explanations outside of the JSON structure. But don't include markdown format like ```json ... ```

    # TOOL LIST #
    {'nodes': [{'id': 'Token Classification', 'desc': 'Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Translation', 'desc': 'Translation is the task of converting text from one language to another.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Summarization', 'desc': 'Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Question Answering', 'desc': 'Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.', 'input-type': ['text', 'text'], 'output-type': ['text']}, {'id': 'Conversational', 'desc': 'Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Text Generation', 'desc': 'Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Sentence Similarity', 'desc': 'Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.', 'input-type': ['text', 'text'], 'output-type': []}, {'id': 'Tabular Classification', 'desc': 'Tabular classification is the task of classifying a table (in Image format).', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Object Detection', 'desc': 'Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image Classification', 'desc': 'Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image-to-Image', 'desc': 'Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Image-to-Text', 'desc': 'Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Text-to-Image', 'desc': 'Generates images from input text. These models can be used to generate images based on text prompts.', 'input-type': ['text'], 'output-type': ['image']}, {'id': 'Text-to-Video', 'desc': 'Generates videos from input text. These models can be used to generate videos based on text prompts.', 'input-type': ['text'], 'output-type': ['video']}, {'id': 'Visual Question Answering', 'desc': 'Visual Question Answering is the task of answering questions based on an image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Document Question Answering', 'desc': 'Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Image Segmentation', 'desc': 'Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Depth Estimation', 'desc': 'Depth estimation is the task of predicting depth of the objects present in an image.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Text-to-Speech', 'desc': 'Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.', 'input-type': ['text'], 'output-type': ['audio']}, {'id': 'Automatic Speech Recognition', 'desc': 'Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Audio-to-Audio', 'desc': 'Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.', 'input-type': ['audio'], 'output-type': ['audio']}, {'id': 'Audio Classification', 'desc': 'Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Image Editing', 'desc': 'Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.', 'input-type': ['text', 'image'], 'output-type': ['image']}]}

    ### TASKS ###
 {'tasks': [{'id': '17301639', 'description': "I've written a fun message for my party invite: 'I am so excited for the party tonight!'. I'm curious, what sort of emotion do you think this message would convey if it were spoken? Could you turn it into an audio message and analyze it for me?"}, {'id': '25351667', 'description': "I was browsing through some old files and came across 'example.jpg'. However, my system fails to display images at the moment. Could you help me understand the image by telling me the color of the dominant object in it using audio format?"}, {'id': '13763755', 'description': "I've got this message 'I am just thrilled about the party later today'. It would be interesting to transform it into an audio format and figure out what would be the primary emotion expressed in the converted audio. Do you think you could help with that?"}]}

# RESPONSE FORMAT #
Your response must be a JSON object with the following structure:
{
  "task_ids": ["task_id_1", "task_id_2", ...],
  "plan": {
    "task_nodes": [
      {
        "node_id": "node_1",
        "tool_name": "tool_name_here",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "value1", "param2": "value2" } },
            { "task_id": "task_id_2", "params": { "param1": "value3", "param2": "value4" } },
        ],
        "dependencies": [],
        "original_task_ids": ["task_id_1", "task_id_2"]
      },
      {
        "node_id": "node_2",
        "tool_name": "another_tool",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "<node_1>", "param2": "value2" } },
        ],
        "dependencies": ["node_1"],
        "original_task_ids": ["task_id_1"]
      }
    ]
  }
}



You are an expert AI assistant specializing in planning and orchestrating tool usage. Your task is to generate a comprehensive tool invocation plan based on a series of user-defined goals.

You must follow these instructions strictly:
1.  **Analyze the Goal**: Carefully examine all the tasks described in the # GOAL # section.
2.  **Consult the Tool List**: Refer to the available tools defined in the # TOOL LIST # section to determine the appropriate tool for each step of each task, you can only use tool define in the # TOOL LIST # section.
3.  **Construct a Unified Plan**: Create a single, unified directed acyclic graph (DAG) that represents the execution plan for all tasks, you need plan use tools as little as possible.
4.  **Reuse Common Nodes**: If multiple tasks require the exact same tool call (not must have exact parameters, Tools Model will batch two request with different parameters), you MUST reuse the same node for that operation in your plan. This is critical for efficiency.
5.  **Strict JSON Output**: The final output MUST be a single, valid JSON object. Do not include any text or explanations outside of the JSON structure. But don't include markdown format like ```json ... ```

    # TOOL LIST #
    {'nodes': [{'id': 'Token Classification', 'desc': 'Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Translation', 'desc': 'Translation is the task of converting text from one language to another.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Summarization', 'desc': 'Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Question Answering', 'desc': 'Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.', 'input-type': ['text', 'text'], 'output-type': ['text']}, {'id': 'Conversational', 'desc': 'Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Text Generation', 'desc': 'Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Sentence Similarity', 'desc': 'Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.', 'input-type': ['text', 'text'], 'output-type': []}, {'id': 'Tabular Classification', 'desc': 'Tabular classification is the task of classifying a table (in Image format).', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Object Detection', 'desc': 'Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image Classification', 'desc': 'Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image-to-Image', 'desc': 'Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Image-to-Text', 'desc': 'Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Text-to-Image', 'desc': 'Generates images from input text. These models can be used to generate images based on text prompts.', 'input-type': ['text'], 'output-type': ['image']}, {'id': 'Text-to-Video', 'desc': 'Generates videos from input text. These models can be used to generate videos based on text prompts.', 'input-type': ['text'], 'output-type': ['video']}, {'id': 'Visual Question Answering', 'desc': 'Visual Question Answering is the task of answering questions based on an image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Document Question Answering', 'desc': 'Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Image Segmentation', 'desc': 'Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Depth Estimation', 'desc': 'Depth estimation is the task of predicting depth of the objects present in an image.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Text-to-Speech', 'desc': 'Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.', 'input-type': ['text'], 'output-type': ['audio']}, {'id': 'Automatic Speech Recognition', 'desc': 'Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Audio-to-Audio', 'desc': 'Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.', 'input-type': ['audio'], 'output-type': ['audio']}, {'id': 'Audio Classification', 'desc': 'Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Image Editing', 'desc': 'Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.', 'input-type': ['text', 'image'], 'output-type': ['image']}]}

    ### TASKS ###
 {'tasks': [{'id': '26145595', 'description': "I have an image named 'example.jpg' with a car in it. Can you help me to change the color of that car to blue? After the color modification, please assist me in estimating the depth of the objects in the edited image."}, {'id': '32194757', 'description': "I'm working on a design project and found an image, 'example.jpg', that encapsulates the mood I want to set. Could you assist me in creating a new image that maintains the same theme but incorporates the color '#FF5733'? Additionally, I'd like to evaluate how various objects and features interact spatially in the new result. Could you help me estimate the depth of the new image?"}, {'id': '30137796', 'description': "I'm trying to set up a digital art project, where I need to manipulate the colors of certain objects in an image. Could you help change the color of car to red and the house to blue in the image “example.jpg”. After the modifications, could you also help me with estimating the depth of the objects?"}]}

# RESPONSE FORMAT #
Your response must be a JSON object with the following structure:
{
  "task_ids": ["task_id_1", "task_id_2", ...],
  "plan": {
    "task_nodes": [
      {
        "node_id": "node_1",
        "tool_name": "tool_name_here",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "value1", "param2": "value2" } },
            { "task_id": "task_id_2", "params": { "param1": "value3", "param2": "value4" } },
        ],
        "dependencies": [],
        "original_task_ids": ["task_id_1", "task_id_2"]
      },
      {
        "node_id": "node_2",
        "tool_name": "another_tool",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "<node_1>", "param2": "value2" } },
        ],
        "dependencies": ["node_1"],
        "original_task_ids": ["task_id_1"]
      }
    ]
  }
}



You are an expert AI assistant specializing in planning and orchestrating tool usage. Your task is to generate a comprehensive tool invocation plan based on a series of user-defined goals.

You must follow these instructions strictly:
1.  **Analyze the Goal**: Carefully examine all the tasks described in the # GOAL # section.
2.  **Consult the Tool List**: Refer to the available tools defined in the # TOOL LIST # section to determine the appropriate tool for each step of each task, you can only use tool define in the # TOOL LIST # section.
3.  **Construct a Unified Plan**: Create a single, unified directed acyclic graph (DAG) that represents the execution plan for all tasks, you need plan use tools as little as possible.
4.  **Reuse Common Nodes**: If multiple tasks require the exact same tool call (not must have exact parameters, Tools Model will batch two request with different parameters), you MUST reuse the same node for that operation in your plan. This is critical for efficiency.
5.  **Strict JSON Output**: The final output MUST be a single, valid JSON object. Do not include any text or explanations outside of the JSON structure. But don't include markdown format like ```json ... ```

    # TOOL LIST #
    {'nodes': [{'id': 'Token Classification', 'desc': 'Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Translation', 'desc': 'Translation is the task of converting text from one language to another.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Summarization', 'desc': 'Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Question Answering', 'desc': 'Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.', 'input-type': ['text', 'text'], 'output-type': ['text']}, {'id': 'Conversational', 'desc': 'Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Text Generation', 'desc': 'Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Sentence Similarity', 'desc': 'Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.', 'input-type': ['text', 'text'], 'output-type': []}, {'id': 'Tabular Classification', 'desc': 'Tabular classification is the task of classifying a table (in Image format).', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Object Detection', 'desc': 'Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image Classification', 'desc': 'Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image-to-Image', 'desc': 'Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Image-to-Text', 'desc': 'Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Text-to-Image', 'desc': 'Generates images from input text. These models can be used to generate images based on text prompts.', 'input-type': ['text'], 'output-type': ['image']}, {'id': 'Text-to-Video', 'desc': 'Generates videos from input text. These models can be used to generate videos based on text prompts.', 'input-type': ['text'], 'output-type': ['video']}, {'id': 'Visual Question Answering', 'desc': 'Visual Question Answering is the task of answering questions based on an image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Document Question Answering', 'desc': 'Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Image Segmentation', 'desc': 'Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Depth Estimation', 'desc': 'Depth estimation is the task of predicting depth of the objects present in an image.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Text-to-Speech', 'desc': 'Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.', 'input-type': ['text'], 'output-type': ['audio']}, {'id': 'Automatic Speech Recognition', 'desc': 'Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Audio-to-Audio', 'desc': 'Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.', 'input-type': ['audio'], 'output-type': ['audio']}, {'id': 'Audio Classification', 'desc': 'Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Image Editing', 'desc': 'Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.', 'input-type': ['text', 'image'], 'output-type': ['image']}]}

    ### TASKS ###
 {'tasks': [{'id': '33218789', 'description': "Just imagine that I'm about to have a conversation with a French friend but, alas, my grasp of the French language is non-existant. I do however have a text in English: 'How was your weekend? Any interesting plans for the week?' and I need your assistance to convert it into French. Can you help me with that?"}, {'id': '28771371', 'description': "I've come across this fascinating image named 'example.jpg' that contains a striking piece of text. I'm thinking, wouldn't it be interesting if we could formulate a conversational response based on that particular text? Could you assist me with this?"}, {'id': '18911675', 'description': "I've received a photo with text in Spanish and I'm having trouble understanding it. Could you assist me in translating the text from this photo (example.jpg) into English?"}]}

# RESPONSE FORMAT #
Your response must be a JSON object with the following structure:
{
  "task_ids": ["task_id_1", "task_id_2", ...],
  "plan": {
    "task_nodes": [
      {
        "node_id": "node_1",
        "tool_name": "tool_name_here",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "value1", "param2": "value2" } },
            { "task_id": "task_id_2", "params": { "param1": "value3", "param2": "value4" } },
        ],
        "dependencies": [],
        "original_task_ids": ["task_id_1", "task_id_2"]
      },
      {
        "node_id": "node_2",
        "tool_name": "another_tool",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "<node_1>", "param2": "value2" } },
        ],
        "dependencies": ["node_1"],
        "original_task_ids": ["task_id_1"]
      }
    ]
  }
}



You are an expert AI assistant specializing in planning and orchestrating tool usage. Your task is to generate a comprehensive tool invocation plan based on a series of user-defined goals.

You must follow these instructions strictly:
1.  **Analyze the Goal**: Carefully examine all the tasks described in the # GOAL # section.
2.  **Consult the Tool List**: Refer to the available tools defined in the # TOOL LIST # section to determine the appropriate tool for each step of each task, you can only use tool define in the # TOOL LIST # section.
3.  **Construct a Unified Plan**: Create a single, unified directed acyclic graph (DAG) that represents the execution plan for all tasks, you need plan use tools as little as possible.
4.  **Reuse Common Nodes**: If multiple tasks require the exact same tool call (not must have exact parameters, Tools Model will batch two request with different parameters), you MUST reuse the same node for that operation in your plan. This is critical for efficiency.
5.  **Strict JSON Output**: The final output MUST be a single, valid JSON object. Do not include any text or explanations outside of the JSON structure. But don't include markdown format like ```json ... ```

    # TOOL LIST #
    {'nodes': [{'id': 'Token Classification', 'desc': 'Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Translation', 'desc': 'Translation is the task of converting text from one language to another.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Summarization', 'desc': 'Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Question Answering', 'desc': 'Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.', 'input-type': ['text', 'text'], 'output-type': ['text']}, {'id': 'Conversational', 'desc': 'Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Text Generation', 'desc': 'Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Sentence Similarity', 'desc': 'Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.', 'input-type': ['text', 'text'], 'output-type': []}, {'id': 'Tabular Classification', 'desc': 'Tabular classification is the task of classifying a table (in Image format).', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Object Detection', 'desc': 'Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image Classification', 'desc': 'Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image-to-Image', 'desc': 'Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Image-to-Text', 'desc': 'Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Text-to-Image', 'desc': 'Generates images from input text. These models can be used to generate images based on text prompts.', 'input-type': ['text'], 'output-type': ['image']}, {'id': 'Text-to-Video', 'desc': 'Generates videos from input text. These models can be used to generate videos based on text prompts.', 'input-type': ['text'], 'output-type': ['video']}, {'id': 'Visual Question Answering', 'desc': 'Visual Question Answering is the task of answering questions based on an image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Document Question Answering', 'desc': 'Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Image Segmentation', 'desc': 'Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Depth Estimation', 'desc': 'Depth estimation is the task of predicting depth of the objects present in an image.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Text-to-Speech', 'desc': 'Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.', 'input-type': ['text'], 'output-type': ['audio']}, {'id': 'Automatic Speech Recognition', 'desc': 'Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Audio-to-Audio', 'desc': 'Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.', 'input-type': ['audio'], 'output-type': ['audio']}, {'id': 'Audio Classification', 'desc': 'Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Image Editing', 'desc': 'Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.', 'input-type': ['text', 'image'], 'output-type': ['image']}]}

    ### TASKS ###
 {'tasks': [{'id': '84151584', 'description': "I've been reading a study material about World War II, however, I would prefer to hear the answer rather than read. Can you help me find out when World War II ended from the given document: 'World War II began in 1939 and ended in 1945...' and convert the answer into audio?"}, {'id': '10394584', 'description': "I've got an image named 'example.jpg' containing some text. Could you assist me by decoding the text and figuring out the main theme of the text?"}, {'id': '21508866', 'description': "Consider the following scenario: there's a text in an image 'example.jpg' but the current text color and background are not suitable for my needs. Could you assist me in modifying the colors of the image? I would like to have the text in blue on a white background. Additionally, once we have the modified image, could you also transcribe the text from the image?"}]}

# RESPONSE FORMAT #
Your response must be a JSON object with the following structure:
{
  "task_ids": ["task_id_1", "task_id_2", ...],
  "plan": {
    "task_nodes": [
      {
        "node_id": "node_1",
        "tool_name": "tool_name_here",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "value1", "param2": "value2" } },
            { "task_id": "task_id_2", "params": { "param1": "value3", "param2": "value4" } },
        ],
        "dependencies": [],
        "original_task_ids": ["task_id_1", "task_id_2"]
      },
      {
        "node_id": "node_2",
        "tool_name": "another_tool",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "<node_1>", "param2": "value2" } },
        ],
        "dependencies": ["node_1"],
        "original_task_ids": ["task_id_1"]
      }
    ]
  }
}



You are an expert AI assistant specializing in planning and orchestrating tool usage. Your task is to generate a comprehensive tool invocation plan based on a series of user-defined goals.

You must follow these instructions strictly:
1.  **Analyze the Goal**: Carefully examine all the tasks described in the # GOAL # section.
2.  **Consult the Tool List**: Refer to the available tools defined in the # TOOL LIST # section to determine the appropriate tool for each step of each task, you can only use tool define in the # TOOL LIST # section.
3.  **Construct a Unified Plan**: Create a single, unified directed acyclic graph (DAG) that represents the execution plan for all tasks, you need plan use tools as little as possible.
4.  **Reuse Common Nodes**: If multiple tasks require the exact same tool call (not must have exact parameters, Tools Model will batch two request with different parameters), you MUST reuse the same node for that operation in your plan. This is critical for efficiency.
5.  **Strict JSON Output**: The final output MUST be a single, valid JSON object. Do not include any text or explanations outside of the JSON structure. But don't include markdown format like ```json ... ```

    # TOOL LIST #
    {'nodes': [{'id': 'Token Classification', 'desc': 'Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Translation', 'desc': 'Translation is the task of converting text from one language to another.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Summarization', 'desc': 'Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Question Answering', 'desc': 'Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.', 'input-type': ['text', 'text'], 'output-type': ['text']}, {'id': 'Conversational', 'desc': 'Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Text Generation', 'desc': 'Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Sentence Similarity', 'desc': 'Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.', 'input-type': ['text', 'text'], 'output-type': []}, {'id': 'Tabular Classification', 'desc': 'Tabular classification is the task of classifying a table (in Image format).', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Object Detection', 'desc': 'Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image Classification', 'desc': 'Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image-to-Image', 'desc': 'Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Image-to-Text', 'desc': 'Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Text-to-Image', 'desc': 'Generates images from input text. These models can be used to generate images based on text prompts.', 'input-type': ['text'], 'output-type': ['image']}, {'id': 'Text-to-Video', 'desc': 'Generates videos from input text. These models can be used to generate videos based on text prompts.', 'input-type': ['text'], 'output-type': ['video']}, {'id': 'Visual Question Answering', 'desc': 'Visual Question Answering is the task of answering questions based on an image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Document Question Answering', 'desc': 'Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Image Segmentation', 'desc': 'Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Depth Estimation', 'desc': 'Depth estimation is the task of predicting depth of the objects present in an image.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Text-to-Speech', 'desc': 'Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.', 'input-type': ['text'], 'output-type': ['audio']}, {'id': 'Automatic Speech Recognition', 'desc': 'Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Audio-to-Audio', 'desc': 'Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.', 'input-type': ['audio'], 'output-type': ['audio']}, {'id': 'Audio Classification', 'desc': 'Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Image Editing', 'desc': 'Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.', 'input-type': ['text', 'image'], 'output-type': ['image']}]}

    ### TASKS ###
 {'tasks': [{'id': '27221536', 'description': "I've been reading about the progress and advancements in the world of Artificial Intelligence, particularly recent breakthroughs in machine learning, natural language processing, computer vision, the deployment of OpenAI's GPT-3 and the emergence of deepfake technology. Consequently, I've tried to wrap my head around all this by drafting the concepts down but I feel overwhelmed. Could you help to condense this down into a simple, conversational explanation of the key ideas so I can discuss it effectively in my project meeting?"}, {'id': '34008432', 'description': "I have been exploring the practice of meditation and came across an exhaustive article that outlines its benefits such as reduced stress levels, improved concentration, increased self-awareness and so on. However, understanding the main points from the lengthy text is quite challenging. Could you help me to condense the content and provide a summarized version of the article? Could you also generate a fresh paragraph that elaborates on the summary? After that, I would appreciate it if you could furnish a related conversational response that connects with the newly created paragraph. Here is the text of my article: 'Meditation has been practiced for thousands of years, offering numerous benefits to those who engage in its practice. Some key benefits include reduced stress levels, improved concentration, increased self-awareness...'"}, {'id': '21772101', 'description': "I'm prepping for an upcoming interview and I'd like to brush up on my skills. Could you help me create a speech about 'The importance of being punctical', and then analyse it to provide me a suitable conversational response for any questions asked by the interviewer?"}]}

# RESPONSE FORMAT #
Your response must be a JSON object with the following structure:
{
  "task_ids": ["task_id_1", "task_id_2", ...],
  "plan": {
    "task_nodes": [
      {
        "node_id": "node_1",
        "tool_name": "tool_name_here",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "value1", "param2": "value2" } },
            { "task_id": "task_id_2", "params": { "param1": "value3", "param2": "value4" } },
        ],
        "dependencies": [],
        "original_task_ids": ["task_id_1", "task_id_2"]
      },
      {
        "node_id": "node_2",
        "tool_name": "another_tool",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "<node_1>", "param2": "value2" } },
        ],
        "dependencies": ["node_1"],
        "original_task_ids": ["task_id_1"]
      }
    ]
  }
}



You are an expert AI assistant specializing in planning and orchestrating tool usage. Your task is to generate a comprehensive tool invocation plan based on a series of user-defined goals.

You must follow these instructions strictly:
1.  **Analyze the Goal**: Carefully examine all the tasks described in the # GOAL # section.
2.  **Consult the Tool List**: Refer to the available tools defined in the # TOOL LIST # section to determine the appropriate tool for each step of each task, you can only use tool define in the # TOOL LIST # section.
3.  **Construct a Unified Plan**: Create a single, unified directed acyclic graph (DAG) that represents the execution plan for all tasks, you need plan use tools as little as possible.
4.  **Reuse Common Nodes**: If multiple tasks require the exact same tool call (not must have exact parameters, Tools Model will batch two request with different parameters), you MUST reuse the same node for that operation in your plan. This is critical for efficiency.
5.  **Strict JSON Output**: The final output MUST be a single, valid JSON object. Do not include any text or explanations outside of the JSON structure. But don't include markdown format like ```json ... ```

    # TOOL LIST #
    {'nodes': [{'id': 'Token Classification', 'desc': 'Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Translation', 'desc': 'Translation is the task of converting text from one language to another.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Summarization', 'desc': 'Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Question Answering', 'desc': 'Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.', 'input-type': ['text', 'text'], 'output-type': ['text']}, {'id': 'Conversational', 'desc': 'Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Text Generation', 'desc': 'Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Sentence Similarity', 'desc': 'Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.', 'input-type': ['text', 'text'], 'output-type': []}, {'id': 'Tabular Classification', 'desc': 'Tabular classification is the task of classifying a table (in Image format).', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Object Detection', 'desc': 'Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image Classification', 'desc': 'Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image-to-Image', 'desc': 'Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Image-to-Text', 'desc': 'Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Text-to-Image', 'desc': 'Generates images from input text. These models can be used to generate images based on text prompts.', 'input-type': ['text'], 'output-type': ['image']}, {'id': 'Text-to-Video', 'desc': 'Generates videos from input text. These models can be used to generate videos based on text prompts.', 'input-type': ['text'], 'output-type': ['video']}, {'id': 'Visual Question Answering', 'desc': 'Visual Question Answering is the task of answering questions based on an image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Document Question Answering', 'desc': 'Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Image Segmentation', 'desc': 'Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Depth Estimation', 'desc': 'Depth estimation is the task of predicting depth of the objects present in an image.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Text-to-Speech', 'desc': 'Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.', 'input-type': ['text'], 'output-type': ['audio']}, {'id': 'Automatic Speech Recognition', 'desc': 'Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Audio-to-Audio', 'desc': 'Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.', 'input-type': ['audio'], 'output-type': ['audio']}, {'id': 'Audio Classification', 'desc': 'Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Image Editing', 'desc': 'Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.', 'input-type': ['text', 'image'], 'output-type': ['image']}]}

    ### TASKS ###
 {'tasks': [{'id': '14773412', 'description': "In my role, I frequently work with images and need to understand each element's depth for analysis. For instance, I have an image named 'example.jpg' right now. Can you help me estimate the depth of the objects in this image? Also, once the depth estimation is done, can you assist in detecting the objects and determining their bounding boxes within that depth-estimated image?"}, {'id': '33218789', 'description': "Just imagine that I'm about to have a conversation with a French friend but, alas, my grasp of the French language is non-existant. I do however have a text in English: 'How was your weekend? Any interesting plans for the week?' and I need your assistance to convert it into French. Can you help me with that?"}, {'id': '25541238', 'description': "I have a document contained in 'example.jpg' and am curious about the primary focus or main subject of this document. Could you extract this information and provide it to me in French?"}]}

# RESPONSE FORMAT #
Your response must be a JSON object with the following structure:
{
  "task_ids": ["task_id_1", "task_id_2", ...],
  "plan": {
    "task_nodes": [
      {
        "node_id": "node_1",
        "tool_name": "tool_name_here",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "value1", "param2": "value2" } },
            { "task_id": "task_id_2", "params": { "param1": "value3", "param2": "value4" } },
        ],
        "dependencies": [],
        "original_task_ids": ["task_id_1", "task_id_2"]
      },
      {
        "node_id": "node_2",
        "tool_name": "another_tool",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "<node_1>", "param2": "value2" } },
        ],
        "dependencies": ["node_1"],
        "original_task_ids": ["task_id_1"]
      }
    ]
  }
}



You are an expert AI assistant specializing in planning and orchestrating tool usage. Your task is to generate a comprehensive tool invocation plan based on a series of user-defined goals.

You must follow these instructions strictly:
1.  **Analyze the Goal**: Carefully examine all the tasks described in the # GOAL # section.
2.  **Consult the Tool List**: Refer to the available tools defined in the # TOOL LIST # section to determine the appropriate tool for each step of each task, you can only use tool define in the # TOOL LIST # section.
3.  **Construct a Unified Plan**: Create a single, unified directed acyclic graph (DAG) that represents the execution plan for all tasks, you need plan use tools as little as possible.
4.  **Reuse Common Nodes**: If multiple tasks require the exact same tool call (not must have exact parameters, Tools Model will batch two request with different parameters), you MUST reuse the same node for that operation in your plan. This is critical for efficiency.
5.  **Strict JSON Output**: The final output MUST be a single, valid JSON object. Do not include any text or explanations outside of the JSON structure. But don't include markdown format like ```json ... ```

    # TOOL LIST #
    {'nodes': [{'id': 'Token Classification', 'desc': 'Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Translation', 'desc': 'Translation is the task of converting text from one language to another.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Summarization', 'desc': 'Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Question Answering', 'desc': 'Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.', 'input-type': ['text', 'text'], 'output-type': ['text']}, {'id': 'Conversational', 'desc': 'Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Text Generation', 'desc': 'Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Sentence Similarity', 'desc': 'Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.', 'input-type': ['text', 'text'], 'output-type': []}, {'id': 'Tabular Classification', 'desc': 'Tabular classification is the task of classifying a table (in Image format).', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Object Detection', 'desc': 'Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image Classification', 'desc': 'Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image-to-Image', 'desc': 'Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Image-to-Text', 'desc': 'Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Text-to-Image', 'desc': 'Generates images from input text. These models can be used to generate images based on text prompts.', 'input-type': ['text'], 'output-type': ['image']}, {'id': 'Text-to-Video', 'desc': 'Generates videos from input text. These models can be used to generate videos based on text prompts.', 'input-type': ['text'], 'output-type': ['video']}, {'id': 'Visual Question Answering', 'desc': 'Visual Question Answering is the task of answering questions based on an image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Document Question Answering', 'desc': 'Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Image Segmentation', 'desc': 'Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Depth Estimation', 'desc': 'Depth estimation is the task of predicting depth of the objects present in an image.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Text-to-Speech', 'desc': 'Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.', 'input-type': ['text'], 'output-type': ['audio']}, {'id': 'Automatic Speech Recognition', 'desc': 'Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Audio-to-Audio', 'desc': 'Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.', 'input-type': ['audio'], 'output-type': ['audio']}, {'id': 'Audio Classification', 'desc': 'Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Image Editing', 'desc': 'Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.', 'input-type': ['text', 'image'], 'output-type': ['image']}]}

    ### TASKS ###
 {'tasks': [{'id': '31061766', 'description': "In a digital copy of an old photograph 'example.jpg', I've identified some text which appears to be important. Could you assist me in deciphering the text and then use it as a springboard to create an insightful paragraph related to the context of that text?"}, {'id': '25249249', 'description': "I've been looking over an educational document I have about various animals. There's a particular description that's got me curious: 'What kind of animal has black and white fur and enjoys eating bamboo?' Can you help me figure that out and maybe also get an image representation of the said creature based on the document? The document says, 'Giraffes have long necks and live in Africa. Pandas are black and white and eat bamboo. Elephants are large mammals with long trunks. Kangaroos are known for their powerful hind legs and pouches.'"}, {'id': '26046456', 'description': "I've started writing a phrase that goes like this: 'The quick brown fox jumps over the...'. Could you, please, complete this phrase for me and then tell me the color of the fox mentioned in it?"}]}

# RESPONSE FORMAT #
Your response must be a JSON object with the following structure:
{
  "task_ids": ["task_id_1", "task_id_2", ...],
  "plan": {
    "task_nodes": [
      {
        "node_id": "node_1",
        "tool_name": "tool_name_here",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "value1", "param2": "value2" } },
            { "task_id": "task_id_2", "params": { "param1": "value3", "param2": "value4" } },
        ],
        "dependencies": [],
        "original_task_ids": ["task_id_1", "task_id_2"]
      },
      {
        "node_id": "node_2",
        "tool_name": "another_tool",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "<node_1>", "param2": "value2" } },
        ],
        "dependencies": ["node_1"],
        "original_task_ids": ["task_id_1"]
      }
    ]
  }
}



You are an expert AI assistant specializing in planning and orchestrating tool usage. Your task is to generate a comprehensive tool invocation plan based on a series of user-defined goals.

You must follow these instructions strictly:
1.  **Analyze the Goal**: Carefully examine all the tasks described in the # GOAL # section.
2.  **Consult the Tool List**: Refer to the available tools defined in the # TOOL LIST # section to determine the appropriate tool for each step of each task, you can only use tool define in the # TOOL LIST # section.
3.  **Construct a Unified Plan**: Create a single, unified directed acyclic graph (DAG) that represents the execution plan for all tasks, you need plan use tools as little as possible.
4.  **Reuse Common Nodes**: If multiple tasks require the exact same tool call (not must have exact parameters, Tools Model will batch two request with different parameters), you MUST reuse the same node for that operation in your plan. This is critical for efficiency.
5.  **Strict JSON Output**: The final output MUST be a single, valid JSON object. Do not include any text or explanations outside of the JSON structure. But don't include markdown format like ```json ... ```

    # TOOL LIST #
    {'nodes': [{'id': 'Token Classification', 'desc': 'Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Translation', 'desc': 'Translation is the task of converting text from one language to another.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Summarization', 'desc': 'Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Question Answering', 'desc': 'Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.', 'input-type': ['text', 'text'], 'output-type': ['text']}, {'id': 'Conversational', 'desc': 'Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Text Generation', 'desc': 'Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Sentence Similarity', 'desc': 'Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.', 'input-type': ['text', 'text'], 'output-type': []}, {'id': 'Tabular Classification', 'desc': 'Tabular classification is the task of classifying a table (in Image format).', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Object Detection', 'desc': 'Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image Classification', 'desc': 'Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image-to-Image', 'desc': 'Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Image-to-Text', 'desc': 'Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Text-to-Image', 'desc': 'Generates images from input text. These models can be used to generate images based on text prompts.', 'input-type': ['text'], 'output-type': ['image']}, {'id': 'Text-to-Video', 'desc': 'Generates videos from input text. These models can be used to generate videos based on text prompts.', 'input-type': ['text'], 'output-type': ['video']}, {'id': 'Visual Question Answering', 'desc': 'Visual Question Answering is the task of answering questions based on an image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Document Question Answering', 'desc': 'Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Image Segmentation', 'desc': 'Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Depth Estimation', 'desc': 'Depth estimation is the task of predicting depth of the objects present in an image.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Text-to-Speech', 'desc': 'Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.', 'input-type': ['text'], 'output-type': ['audio']}, {'id': 'Automatic Speech Recognition', 'desc': 'Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Audio-to-Audio', 'desc': 'Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.', 'input-type': ['audio'], 'output-type': ['audio']}, {'id': 'Audio Classification', 'desc': 'Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Image Editing', 'desc': 'Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.', 'input-type': ['text', 'image'], 'output-type': ['image']}]}

    ### TASKS ###
 {'tasks': [{'id': '11900641', 'description': "I've been going through several photos of mine, including one named example.jpg, and have been thinking of lending them a classic, vintage look. So, let's start with the image example.jpg; could you assist me in applying a vintage-style transformation to it? Also, I'd really appreciate it if you can give me insight into what is the primary object in the vintage-styled image?"}, {'id': '10925281', 'description': "I recently stumbled upon an old photograph labeled 'example.jpg'. It represents a beautiful building, and I'm wishing I can visualize it in a modern photographic style. Then, I'd love to learn the actual name of this majestically depicted building. Could you assist me in accomplishing both these tasks?"}, {'id': '10667740', 'description': "I've come across a table captured in the image example.jpg and it possibly mentions the term 'COVID-19' several times. Could you assist me in figuring out how many times 'COVID-19' appears in the table?"}]}

# RESPONSE FORMAT #
Your response must be a JSON object with the following structure:
{
  "task_ids": ["task_id_1", "task_id_2", ...],
  "plan": {
    "task_nodes": [
      {
        "node_id": "node_1",
        "tool_name": "tool_name_here",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "value1", "param2": "value2" } },
            { "task_id": "task_id_2", "params": { "param1": "value3", "param2": "value4" } },
        ],
        "dependencies": [],
        "original_task_ids": ["task_id_1", "task_id_2"]
      },
      {
        "node_id": "node_2",
        "tool_name": "another_tool",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "<node_1>", "param2": "value2" } },
        ],
        "dependencies": ["node_1"],
        "original_task_ids": ["task_id_1"]
      }
    ]
  }
}



You are an expert AI assistant specializing in planning and orchestrating tool usage. Your task is to generate a comprehensive tool invocation plan based on a series of user-defined goals.

You must follow these instructions strictly:
1.  **Analyze the Goal**: Carefully examine all the tasks described in the # GOAL # section.
2.  **Consult the Tool List**: Refer to the available tools defined in the # TOOL LIST # section to determine the appropriate tool for each step of each task, you can only use tool define in the # TOOL LIST # section.
3.  **Construct a Unified Plan**: Create a single, unified directed acyclic graph (DAG) that represents the execution plan for all tasks, you need plan use tools as little as possible.
4.  **Reuse Common Nodes**: If multiple tasks require the exact same tool call (not must have exact parameters, Tools Model will batch two request with different parameters), you MUST reuse the same node for that operation in your plan. This is critical for efficiency.
5.  **Strict JSON Output**: The final output MUST be a single, valid JSON object. Do not include any text or explanations outside of the JSON structure. But don't include markdown format like ```json ... ```

    # TOOL LIST #
    {'nodes': [{'id': 'Token Classification', 'desc': 'Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Translation', 'desc': 'Translation is the task of converting text from one language to another.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Summarization', 'desc': 'Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Question Answering', 'desc': 'Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.', 'input-type': ['text', 'text'], 'output-type': ['text']}, {'id': 'Conversational', 'desc': 'Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Text Generation', 'desc': 'Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Sentence Similarity', 'desc': 'Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.', 'input-type': ['text', 'text'], 'output-type': []}, {'id': 'Tabular Classification', 'desc': 'Tabular classification is the task of classifying a table (in Image format).', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Object Detection', 'desc': 'Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image Classification', 'desc': 'Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image-to-Image', 'desc': 'Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Image-to-Text', 'desc': 'Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Text-to-Image', 'desc': 'Generates images from input text. These models can be used to generate images based on text prompts.', 'input-type': ['text'], 'output-type': ['image']}, {'id': 'Text-to-Video', 'desc': 'Generates videos from input text. These models can be used to generate videos based on text prompts.', 'input-type': ['text'], 'output-type': ['video']}, {'id': 'Visual Question Answering', 'desc': 'Visual Question Answering is the task of answering questions based on an image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Document Question Answering', 'desc': 'Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Image Segmentation', 'desc': 'Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Depth Estimation', 'desc': 'Depth estimation is the task of predicting depth of the objects present in an image.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Text-to-Speech', 'desc': 'Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.', 'input-type': ['text'], 'output-type': ['audio']}, {'id': 'Automatic Speech Recognition', 'desc': 'Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Audio-to-Audio', 'desc': 'Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.', 'input-type': ['audio'], 'output-type': ['audio']}, {'id': 'Audio Classification', 'desc': 'Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Image Editing', 'desc': 'Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.', 'input-type': ['text', 'image'], 'output-type': ['image']}]}

    ### TASKS ###
 {'tasks': [{'id': '33792932', 'description': "I came across a photograph named example.jpg with a car in it. Can you help me identify the car's color? Also, could you generate a passage that paints a vivid image of the car and subsequently formulate a conversational response derived from the generated text?"}, {'id': '21772101', 'description': "I'm prepping for an upcoming interview and I'd like to brush up on my skills. Could you help me create a speech about 'The importance of being punctical', and then analyse it to provide me a suitable conversational response for any questions asked by the interviewer?"}, {'id': '29700804', 'description': "I've taken a photo 'example.jpg' which is filled with various elements and I'm finding it hard to identify all of them. Could you assist me in detecting the objects in the 'example.jpg' and provide a brief description for each of them?"}]}

# RESPONSE FORMAT #
Your response must be a JSON object with the following structure:
{
  "task_ids": ["task_id_1", "task_id_2", ...],
  "plan": {
    "task_nodes": [
      {
        "node_id": "node_1",
        "tool_name": "tool_name_here",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "value1", "param2": "value2" } },
            { "task_id": "task_id_2", "params": { "param1": "value3", "param2": "value4" } },
        ],
        "dependencies": [],
        "original_task_ids": ["task_id_1", "task_id_2"]
      },
      {
        "node_id": "node_2",
        "tool_name": "another_tool",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "<node_1>", "param2": "value2" } },
        ],
        "dependencies": ["node_1"],
        "original_task_ids": ["task_id_1"]
      }
    ]
  }
}



You are an expert AI assistant specializing in planning and orchestrating tool usage. Your task is to generate a comprehensive tool invocation plan based on a series of user-defined goals.

You must follow these instructions strictly:
1.  **Analyze the Goal**: Carefully examine all the tasks described in the # GOAL # section.
2.  **Consult the Tool List**: Refer to the available tools defined in the # TOOL LIST # section to determine the appropriate tool for each step of each task, you can only use tool define in the # TOOL LIST # section.
3.  **Construct a Unified Plan**: Create a single, unified directed acyclic graph (DAG) that represents the execution plan for all tasks, you need plan use tools as little as possible.
4.  **Reuse Common Nodes**: If multiple tasks require the exact same tool call (not must have exact parameters, Tools Model will batch two request with different parameters), you MUST reuse the same node for that operation in your plan. This is critical for efficiency.
5.  **Strict JSON Output**: The final output MUST be a single, valid JSON object. Do not include any text or explanations outside of the JSON structure. But don't include markdown format like ```json ... ```

    # TOOL LIST #
    {'nodes': [{'id': 'Token Classification', 'desc': 'Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Translation', 'desc': 'Translation is the task of converting text from one language to another.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Summarization', 'desc': 'Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Question Answering', 'desc': 'Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.', 'input-type': ['text', 'text'], 'output-type': ['text']}, {'id': 'Conversational', 'desc': 'Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Text Generation', 'desc': 'Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Sentence Similarity', 'desc': 'Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.', 'input-type': ['text', 'text'], 'output-type': []}, {'id': 'Tabular Classification', 'desc': 'Tabular classification is the task of classifying a table (in Image format).', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Object Detection', 'desc': 'Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image Classification', 'desc': 'Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image-to-Image', 'desc': 'Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Image-to-Text', 'desc': 'Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Text-to-Image', 'desc': 'Generates images from input text. These models can be used to generate images based on text prompts.', 'input-type': ['text'], 'output-type': ['image']}, {'id': 'Text-to-Video', 'desc': 'Generates videos from input text. These models can be used to generate videos based on text prompts.', 'input-type': ['text'], 'output-type': ['video']}, {'id': 'Visual Question Answering', 'desc': 'Visual Question Answering is the task of answering questions based on an image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Document Question Answering', 'desc': 'Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Image Segmentation', 'desc': 'Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Depth Estimation', 'desc': 'Depth estimation is the task of predicting depth of the objects present in an image.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Text-to-Speech', 'desc': 'Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.', 'input-type': ['text'], 'output-type': ['audio']}, {'id': 'Automatic Speech Recognition', 'desc': 'Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Audio-to-Audio', 'desc': 'Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.', 'input-type': ['audio'], 'output-type': ['audio']}, {'id': 'Audio Classification', 'desc': 'Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Image Editing', 'desc': 'Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.', 'input-type': ['text', 'image'], 'output-type': ['image']}]}

    ### TASKS ###
 {'tasks': [{'id': '67101251', 'description': 'I have an image named example.jpg containing a main object with a blue tint. Could you help me turn that blue into a red color?'}, {'id': '18267263', 'description': "I've just uploaded an image titled 'example.jpg'. Can you analyze this image and determine what's in it, and then compare this with a statement I'm thinking of: 'Beautiful mountains and a lake'. Can you tell me how similar they are?"}, {'id': '53887645', 'description': "I have an image called example.jpg and I suspect it represents a beautiful sunset. Could you help me verify if this is indeed the case by checking the similarity between the image content and the phrase 'a beautiful sunset'?"}]}

# RESPONSE FORMAT #
Your response must be a JSON object with the following structure:
{
  "task_ids": ["task_id_1", "task_id_2", ...],
  "plan": {
    "task_nodes": [
      {
        "node_id": "node_1",
        "tool_name": "tool_name_here",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "value1", "param2": "value2" } },
            { "task_id": "task_id_2", "params": { "param1": "value3", "param2": "value4" } },
        ],
        "dependencies": [],
        "original_task_ids": ["task_id_1", "task_id_2"]
      },
      {
        "node_id": "node_2",
        "tool_name": "another_tool",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "<node_1>", "param2": "value2" } },
        ],
        "dependencies": ["node_1"],
        "original_task_ids": ["task_id_1"]
      }
    ]
  }
}



You are an expert AI assistant specializing in planning and orchestrating tool usage. Your task is to generate a comprehensive tool invocation plan based on a series of user-defined goals.

You must follow these instructions strictly:
1.  **Analyze the Goal**: Carefully examine all the tasks described in the # GOAL # section.
2.  **Consult the Tool List**: Refer to the available tools defined in the # TOOL LIST # section to determine the appropriate tool for each step of each task, you can only use tool define in the # TOOL LIST # section.
3.  **Construct a Unified Plan**: Create a single, unified directed acyclic graph (DAG) that represents the execution plan for all tasks, you need plan use tools as little as possible.
4.  **Reuse Common Nodes**: If multiple tasks require the exact same tool call (not must have exact parameters, Tools Model will batch two request with different parameters), you MUST reuse the same node for that operation in your plan. This is critical for efficiency.
5.  **Strict JSON Output**: The final output MUST be a single, valid JSON object. Do not include any text or explanations outside of the JSON structure. But don't include markdown format like ```json ... ```

    # TOOL LIST #
    {'nodes': [{'id': 'Token Classification', 'desc': 'Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Translation', 'desc': 'Translation is the task of converting text from one language to another.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Summarization', 'desc': 'Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Question Answering', 'desc': 'Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document.', 'input-type': ['text', 'text'], 'output-type': ['text']}, {'id': 'Conversational', 'desc': 'Conversational response modelling is the task of generating conversational text that is relevant, coherent and knowledgable given a prompt. These models have applications in chatbots, and as a part of voice assistants', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Text Generation', 'desc': 'Generating text is the task of producing new text. These models can, for example, fill in incomplete text or paraphrase.', 'input-type': ['text'], 'output-type': ['text']}, {'id': 'Sentence Similarity', 'desc': 'Sentence Similarity is the task of determining how similar two texts are. This task is particularly useful for information retrieval and clustering/grouping.', 'input-type': ['text', 'text'], 'output-type': []}, {'id': 'Tabular Classification', 'desc': 'Tabular classification is the task of classifying a table (in Image format).', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Object Detection', 'desc': 'Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image Classification', 'desc': 'Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Image-to-Image', 'desc': 'Image-to-image is the task of transforming a source image to match the characteristics of a target image or a target image domain. Any image manipulation and enhancement is possible with image to image models.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Image-to-Text', 'desc': 'Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.', 'input-type': ['image'], 'output-type': ['text']}, {'id': 'Text-to-Image', 'desc': 'Generates images from input text. These models can be used to generate images based on text prompts.', 'input-type': ['text'], 'output-type': ['image']}, {'id': 'Text-to-Video', 'desc': 'Generates videos from input text. These models can be used to generate videos based on text prompts.', 'input-type': ['text'], 'output-type': ['video']}, {'id': 'Visual Question Answering', 'desc': 'Visual Question Answering is the task of answering questions based on an image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Document Question Answering', 'desc': 'Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.', 'input-type': ['image', 'text'], 'output-type': ['text']}, {'id': 'Image Segmentation', 'desc': 'Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Depth Estimation', 'desc': 'Depth estimation is the task of predicting depth of the objects present in an image.', 'input-type': ['image'], 'output-type': ['image']}, {'id': 'Text-to-Speech', 'desc': 'Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.', 'input-type': ['text'], 'output-type': ['audio']}, {'id': 'Automatic Speech Recognition', 'desc': 'Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Audio-to-Audio', 'desc': 'Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.', 'input-type': ['audio'], 'output-type': ['audio']}, {'id': 'Audio Classification', 'desc': 'Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.', 'input-type': ['audio'], 'output-type': ['text']}, {'id': 'Image Editing', 'desc': 'Image editing is the task of modifying an image to match a given text description. It can be used to modify the attributes of an image, such as the color of an object or the background.', 'input-type': ['text', 'image'], 'output-type': ['image']}]}

    ### TASKS ###
 {'tasks': [{'id': '14843300', 'description': "I've recorded some specifications regarding modifications I'd like to make on the image 'example.jpg' in an audio file 'example.wav'. Could you interpret my voice instruction and apply the modifications to the image accordingly?"}, {'id': '14000781', 'description': "I've got this image called 'example.jpg' that has a text which speaks of the changes to be made on the image. Can you modify the image based on the specifications mentioned in the text within it?"}, {'id': '19736753', 'description': 'I seem to be having difficulty understanding the content of a document in an image file, example.jpg. Could you help me dissect it, pull out any text content it may contain, and help me understand the core topic of the document?'}]}

# RESPONSE FORMAT #
Your response must be a JSON object with the following structure:
{
  "task_ids": ["task_id_1", "task_id_2", ...],
  "plan": {
    "task_nodes": [
      {
        "node_id": "node_1",
        "tool_name": "tool_name_here",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "value1", "param2": "value2" } },
            { "task_id": "task_id_2", "params": { "param1": "value3", "param2": "value4" } },
        ],
        "dependencies": [],
        "original_task_ids": ["task_id_1", "task_id_2"]
      },
      {
        "node_id": "node_2",
        "tool_name": "another_tool",
        "parameters": [
            { "task_id": "task_id_1", "params": { "param1": "<node_1>", "param2": "value2" } },
        ],
        "dependencies": ["node_1"],
        "original_task_ids": ["task_id_1"]
      }
    ]
  }
}


